FROM docker.m.daocloud.io/vllm/vllm-openai:v0.11.1

# add vllm tei plugin support
# COPY dist/vllm_tei_plugin-0.10.1.1.tar.gz /tmp/vllm_tei_plugin-0.10.1.1.tar.gz
# RUN pip install /tmp/vllm_tei_plugin-0.10.1.1.tar.gz --no-deps && rm -rf /tmp/vllm_tei_plugin-0.10.1.1.tar.gz

# add vllm kubernetes plugin support
# COPY dist/vllm_kubernetes_plugin-0.1.0.tar.gz /tmp/vllm_kubernetes_plugin-0.1.0.tar.gz
# RUN pip install /tmp/vllm_kubernetes_plugin-0.1.0.tar.gz --no-deps && rm -rf /tmp/vllm_kubernetes_plugin-0.1.0.tar.gz

# add vllm mixed precision model loader
COPY vllm/model_executor/model_loader/mixed_precision_loader.py /usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/mixed_precision_loader.py
COPY vllm/model_executor/model_loader/__init__.py /usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/__init__.py

# add vllm multimodal processing patch
COPY vllm/multimodal/processing.py /usr/local/lib/python3.12/dist-packages/vllm/multimodal/processing.py

ENTRYPOINT ["vllm", "serve"]
